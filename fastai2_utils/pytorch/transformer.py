# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_pytorch.transformer.ipynb (unless otherwise specified).

__all__ = ['gen_key_padding_mask', 'gen_lm_mask', 'BatchFirstTransformerEncoder', 'BatchFirstTransformerDecoder',
           'BatchFirstMultiheadAttention', 'CrossAttention']

# Cell
from fastai2.basics import *

# Cell
def gen_key_padding_mask(input_ids, pad_id):
    ''' Returns ByteTensor where True values are positions that contain pad_id.
        input_ids: (bs, seq_len) returns: (bs, seq_len)
    '''
    device = input_ids.device
    mask = torch.where(input_ids == pad_id, torch.tensor(1, device=device), torch.tensor(0, device=device)).to(device)
    return mask.bool()

# Cell
def gen_lm_mask(tgt_seq_len, device):
    """Generate a square mask for the sequence. The masked positions are filled with float('-inf').
        Unmasked positions are filled with float(0.0).
        ex: tgt_seq_len = 4
        [[0., -inf, -inf, -inf],
        [0., 0., -inf, -inf],
        [0., 0., 0., -inf],
        [0., 0., 0., 0.]])
    """
    mask = (torch.triu(torch.ones(tgt_seq_len, tgt_seq_len)) == 1).transpose(0, 1)
    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
    return mask.to(device)

# Cell
class BatchFirstTransformerEncoder(nn.TransformerEncoder):
    '''
        nn.TransformerEncoder want src be (seq_len, bs, embeded_size) and returns (seq_len, bs, embeded_size),
        just change it to accept batch first input and returns
    '''
    def forward(self, src, *inputs, **kwargs):
        ''' src: (bs, enc_seq_len, embeded_size), returns: (bs, enc_seq_len, embeded_size) '''
        src = src.permute(1, 0, 2) # (enc_seq_len, bs, embeded_size)
        output = super().forward(src, *inputs, **kwargs) # (enc_seq_len, bs, embeded_size)
        return output.permute(1, 0, 2) # (bs, enc_seq_len, embeded_size)

# Cell
class BatchFirstTransformerDecoder(nn.TransformerDecoder):
    '''
        nn.TransformerDecoder want tgt be (seq_len, bs, embeded_size) and returns (seq_len, bs, embeded_size),
        just change it to accept batch first input and returns
    '''
    def forward(self, tgt, memory, *inputs, **kwargs):
        '''
            tgt: (bs, dec_seq_len, embeded_size)
            memory: (bs, enc_seq_len, embeded_size)
            returns: (bs, dec_seq_len, embeded_size)
        '''
        tgt = tgt.permute(1, 0, 2) # (dec_seq_len, bs, embeded_size)
        memory = memory.permute(1, 0, 2) # (enc_seq_len, bs, embeded_size)
        output = super().forward(tgt, memory, *inputs, **kwargs) # (dec_seq_len, bs, embeded_size)
        return output.permute(1, 0, 2) # (bs, dec_seq_len, embeded_size)

# Cell
class BatchFirstMultiheadAttention(nn.MultiheadAttention):
    '''
        Pytorch wants your query, key, value be (seq_len, b, embed_dim) and return (seq_len, b, embed_dim)
        But I like batch-first thing. input: (b, seq_len, embed_dim) output: (b, seq_len, embed_dim)
    '''
    def forward(self, query, key, value, **kwargs):
        '''
        - inputs:
        - query: (bs, tgt_seq_len, embed_dim)
        - key: (bs, src_seq_len, embed_dim)
        - value: (bs, src_seq_len, embed_dim)

        - outputs:
        - attn_output: (bs, tgt_seq_len, embed_dim)
        - attn_weight: (bs, tgt_seq_len, src_seq_len), Averaged weights that averaged over all heads
        '''
        attn_output, attn_weight = super().forward(query.permute(1, 0, 2), key.permute(1, 0, 2), value.permute(1, 0, 2), **kwargs)
        return attn_output.permute(1, 0, 2), attn_weight

# Cell
class CrossAttention(nn.Module):
    def __init__(self, embed_dim, num_heads=1, drop_p=0, num_layers=1):
        super().__init__()
        self.cross_attn_layers = nn.ModuleList(
            [BatchFirstMultiheadAttention(embed_dim, num_heads=num_heads, dropout=drop_p) for _ in range(num_layers)]
        )
    def forward(self, tgt, src, src_key_padding_mask):
        '''
        tgt: (bs, tgt_seq_len, embed_size)
        src: (bs, src_seq_len, embed_size)
        src_key_padding_mask: (bs, src_seq_len)
        returns: output, attn_weight
            output: (bs, tgt_seq_len, embed_dim)
            attn_weight: (bs, tgt_seq_len, src_seq_len)
        '''
        for layer in self.cross_attn_layers:
            tgt, attn_weight = layer(tgt, src, src, key_padding_mask=src_key_padding_mask)
        return tgt, attn_weight