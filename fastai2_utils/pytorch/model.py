# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/03_pytorch.model.ipynb (unless otherwise specified).

__all__ = ['layer_info', 'check_requires_grad', 'set_requires_grad', 'freeze_to']

# Cell
from typing import List
from fastai2.basics import *
from fastai2.callback.hook import *
from fastai2.imports import *

# Cell
def layer_info(model: nn.Module, *sample_inputs):
    "sample_inputs: sample_inputs of your model, only support batch first inputs"
    def _track(m, i, o):
        return (m.__class__.__name__,)+total_params(m)+(apply(lambda x:x.shape, o),)
    layers = [m for m in flatten_model(model)]
    with Hooks(layers, _track) as h:
        _ = model.eval()(*apply(lambda o:o[:1], sample_inputs))
        return sample_inputs,h.stored

# Cell
def _print_shapes(o, bs):
    if isinstance(o, torch.Size): return ' x '.join([str(bs)] + [str(t) for t in o[1:]])
    else: return str([_print_shapes(x, bs) for x in o])

# Cell
@patch
def summary(self: nn.Module, *sample_inputs):
    ''' Print a summary of the model
        sample_inputs: sample inputs of your model, only support batch first inputs
    '''
    sample_inputs,infos = layer_info(self, *sample_inputs)
    n,bs = 64,find_bs(sample_inputs)
    inp_sz = _print_shapes(apply(lambda x:x.shape, sample_inputs), bs)
    res = f"{self.__class__.__name__} (Input shape: {inp_sz})\n"
    res += "=" * n + "\n"
    res += f"{'Layer (type)':<20} {'Output Shape':<20} {'Param #':<10} {'Trainable':<10}\n"
    res += "=" * n + "\n"
    ps,trn_ps = 0,0
    infos = [o for o in infos if o is not None] #see comment in previous cell
    for typ,np,trn,sz in infos:
        if sz is None: continue
        ps += np
        if trn: trn_ps += np
        res += f"{typ:<20} {_print_shapes(sz, bs)[:19]:<20} {np:<10,} {str(trn):<10}\n"
        res += "_" * n + "\n"
    res += f"\nTotal params: {ps:,}\n"
    res += f"Total trainable params: {trn_ps:,}\n"
    res += f"Total non-trainable params: {ps - trn_ps:,}\n\n"
#     res += f"Optimizer used: {self.opt_func}\nLoss function: {self.loss_func}\n\n"
#     if self.opt is not None:
#         res += f"Model " + ("unfrozen\n\n" if self.opt.frozen_idx==0 else f"frozen up to parameter group number {self.opt.frozen_idx}\n\n")
#     res += "Callbacks:\n" + '\n'.join(f"  - {cb}" for cb in sort_by_run(self.cbs))
    return PrettyString(res)


# Cell
def check_requires_grad(layers: List[nn.Module], grad: bool):
    " check whether reauires_grad of all params in layers is grad "
    grads = []
    param_groups = list(map(params, layers)) # [list of params in layer1, list of params in group2, ....]
    for param_group in param_groups:
        for param in param_group:
            grads.append(param.requires_grad)
    if grad==True and all(grads)==True: return True
    elif grad==False and all(grads)==False: return True
    else: return False

# Cell
def set_requires_grad(layers: List[nn.Module], to: bool):
    "set requires_grad of params in layers to to"
    param_groups = list(map(params, layers)) # [list of params in layer1, list of params in group2, ....]
    for param_group in param_groups:
        for param in param_group:
            param.requires_grad_(to)

# Cell
def freeze_to(layers: List[nn.Module], n: int):
    ''' set requires_grad_ to False of layers[:n] and set requires_grad_ to True of layers[n:] '''
    freeze_layers = layers[slice(None, n)]
    unfreeze_layers = layers[slice(n, None)]
    set_requires_grad(freeze_layers, False)
    set_requires_grad(unfreeze_layers, True)