# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/03_pytorch.model.ipynb (unless otherwise specified).

__all__ = ['check_requires_grad', 'set_requires_grad', 'freeze_to']

# Cell
from typing import List
from fastai2.basics import *
from fastai2.callback.hook import *
from fastai2.imports import *

# Cell
def check_requires_grad(layers: List[nn.Module], grad: bool):
    " check whether reauires_grad of all params in layers is grad "
    grads = []
    param_groups = list(map(params, layers)) # [list of params in layer1, list of params in group2, ....]
    for param_group in param_groups:
        for param in param_group:
            grads.append(param.requires_grad)
    if grad==True and all(grads)==True: return True
    elif grad==False and all(grads)==False: return True
    else: return False

# Cell
def set_requires_grad(layers: List[nn.Module], to: bool):
    "set requires_grad of params in layers to to"
    param_groups = list(map(params, layers)) # [list of params in layer1, list of params in group2, ....]
    for param_group in param_groups:
        for param in param_group:
            param.requires_grad_(to)

# Cell
def freeze_to(layers: List[nn.Module], n: int):
    ''' set requires_grad_ to False of layers[:n] and set requires_grad_ to True of layers[n:] '''
    freeze_layers = layers[slice(None, n)]
    unfreeze_layers = layers[slice(n, None)]
    set_requires_grad(freeze_layers, False)
    set_requires_grad(unfreeze_layers, True)