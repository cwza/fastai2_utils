{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai2.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp pytorch.transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Transformer\n",
    "> Utils about Transformer of pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gen_key_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def gen_key_padding_mask(input_ids, pad_id):\n",
    "    ''' Returns ByteTensor where True values are positions that contain pad_id.\n",
    "        input_ids: (bs, seq_len) returns: (bs, seq_len)\n",
    "    '''\n",
    "    device = input_ids.device\n",
    "    mask = torch.where(input_ids == pad_id, torch.tensor(1, device=device), torch.tensor(0, device=device)).to(device)\n",
    "    return mask.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([[12, 11, 0, 0], \n",
    "                          [9, 1, 5, 0]])\n",
    "key_padding_mask = gen_key_padding_mask(input_ids, 0)\n",
    "test_eq(key_padding_mask, torch.tensor([[0, 0, 1, 1],\n",
    "                                        [0, 0, 0, 1]]).bool())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gen_lm_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def gen_lm_mask(tgt_seq_len, device):\n",
    "    \"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
    "        Unmasked positions are filled with float(0.0).\n",
    "        ex: tgt_seq_len = 4\n",
    "        [[0., -inf, -inf, -inf],\n",
    "        [0., 0., -inf, -inf],\n",
    "        [0., 0., 0., -inf],\n",
    "        [0., 0., 0., 0.]])\n",
    "    \"\"\"\n",
    "    mask = (torch.triu(torch.ones(tgt_seq_len, tgt_seq_len)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_mask = gen_lm_mask(4, 'cpu')\n",
    "test_eq(lm_mask, torch.tensor([ [0., float('-inf'), float('-inf'), float('-inf')],\n",
    "                                [0., 0., float('-inf'), float('-inf')],\n",
    "                                [0., 0., 0., float('-inf')],\n",
    "                                [0., 0., 0., 0.]]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchFirstTransformerEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BatchFirstTransformerEncoder(nn.TransformerEncoder):\n",
    "    '''\n",
    "        nn.TransformerEncoder want src be (seq_len, bs, embeded_size) and returns (seq_len, bs, embeded_size),\n",
    "        just change it to accept batch first input and returns\n",
    "    '''\n",
    "    def forward(self, src, *inputs, **kwargs):\n",
    "        ''' src: (bs, enc_seq_len, embeded_size), returns: (bs, enc_seq_len, embeded_size) '''\n",
    "        src = src.permute(1, 0, 2) # (enc_seq_len, bs, embeded_size)\n",
    "        output = super().forward(src, *inputs, **kwargs) # (enc_seq_len, bs, embeded_size)\n",
    "        return output.permute(1, 0, 2) # (bs, enc_seq_len, embeded_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=2)\n",
    "encoder_norm = nn.LayerNorm(normalized_shape=128)\n",
    "encoder = BatchFirstTransformerEncoder(encoder_layer=encoder_layer, num_layers=2, norm=encoder_norm)\n",
    "\n",
    "src = torch.randn((16, 50, 128)) # (bs, enc_seq_len, embeded_size)\n",
    "output = encoder(src) # (bs, enc_seq_len, embeded_size)\n",
    "test_eq(output.shape, (16, 50, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchFirstTransformerDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BatchFirstTransformerDecoder(nn.TransformerDecoder):\n",
    "    '''\n",
    "        nn.TransformerDecoder want tgt be (seq_len, bs, embeded_size) and returns (seq_len, bs, embeded_size),\n",
    "        just change it to accept batch first input and returns\n",
    "    '''\n",
    "    def forward(self, tgt, memory, *inputs, **kwargs):\n",
    "        ''' \n",
    "            tgt: (bs, dec_seq_len, embeded_size)\n",
    "            memory: (bs, enc_seq_len, embeded_size)\n",
    "            returns: (bs, dec_seq_len, embeded_size) \n",
    "        '''\n",
    "        tgt = tgt.permute(1, 0, 2) # (dec_seq_len, bs, embeded_size)\n",
    "        memory = memory.permute(1, 0, 2) # (enc_seq_len, bs, embeded_size)\n",
    "        output = super().forward(tgt, memory, *inputs, **kwargs) # (dec_seq_len, bs, embeded_size)\n",
    "        return output.permute(1, 0, 2) # (bs, dec_seq_len, embeded_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_layer = nn.TransformerDecoderLayer(d_model=128, nhead=2)\n",
    "decoder_norm = nn.LayerNorm(normalized_shape=128)\n",
    "decoder = BatchFirstTransformerDecoder(decoder_layer, num_layers=2, norm=decoder_norm)\n",
    "\n",
    "tgt = torch.randn((16, 40, 128)) # (bs, dec_seq_len)\n",
    "memory = torch.randn((16, 50, 128)) # (bs, enc_seq_len, embeded_size)\n",
    "output = decoder(tgt, memory) # (bs, dec_seq_len, embeded_size)\n",
    "test_eq(output.shape, (16, 40, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchFirstMultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BatchFirstMultiheadAttention(nn.MultiheadAttention):\n",
    "    '''\n",
    "        Pytorch wants your query, key, value be (seq_len, b, embed_dim) and return (seq_len, b, embed_dim)\n",
    "        But I like batch-first thing. input: (b, seq_len, embed_dim) output: (b, seq_len, embed_dim)\n",
    "    '''\n",
    "    def forward(self, query, key, value, **kwargs):\n",
    "        '''\n",
    "        - inputs:\n",
    "        - query: (bs, tgt_seq_len, embed_dim)\n",
    "        - key: (bs, src_seq_len, embed_dim)\n",
    "        - value: (bs, src_seq_len, embed_dim)\n",
    "\n",
    "        - outputs:\n",
    "        - attn_output: (bs, tgt_seq_len, embed_dim)\n",
    "        - attn_weight: (bs, tgt_seq_len, src_seq_len), Averaged weights that averaged over all heads\n",
    "        '''\n",
    "        attn_output, attn_weight = super().forward(query.permute(1, 0, 2), key.permute(1, 0, 2), value.permute(1, 0, 2), **kwargs)\n",
    "        return attn_output.permute(1, 0, 2), attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_attn = BatchFirstMultiheadAttention(embed_dim=128, num_heads=1, dropout=0)\n",
    "query = torch.randn((3, 40, 128))\n",
    "key = torch.randn((3, 50, 128))\n",
    "value = torch.randn((3, 50, 128))\n",
    "attn_output, attn_weight = multi_attn(query, key, value)\n",
    "test_eq(attn_output.shape, (3, 40, 128))\n",
    "test_eq(attn_weight.shape, (3, 40, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrossAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=1, drop_p=0, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.cross_attn_layers = nn.ModuleList(\n",
    "            [BatchFirstMultiheadAttention(embed_dim, num_heads=num_heads, dropout=drop_p) for _ in range(num_layers)]\n",
    "        )\n",
    "    def forward(self, tgt, src, src_key_padding_mask):\n",
    "        '''\n",
    "        tgt: (bs, tgt_seq_len, embed_size)\n",
    "        src: (bs, src_seq_len, embed_size)\n",
    "        src_key_padding_mask: (bs, src_seq_len)\n",
    "        returns: output, attn_weight\n",
    "            output: (bs, tgt_seq_len, embed_dim)\n",
    "            attn_weight: (bs, tgt_seq_len, src_seq_len)\n",
    "        '''\n",
    "        for layer in self.cross_attn_layers:\n",
    "            tgt, attn_weight = layer(tgt, src, src, key_padding_mask=src_key_padding_mask)\n",
    "        return tgt, attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt = torch.randn((3, 40, 768))\n",
    "src = torch.randn((3, 50, 768))\n",
    "src_key_padding_mask = torch.zeros((3, 50)).bool()\n",
    "cross_attn = CrossAttention(embed_dim=768, num_layers=2)\n",
    "\n",
    "cross_attn_out, cross_attn_weight = cross_attn(tgt, src, src_key_padding_mask)\n",
    "test_eq(cross_attn_out.shape, (3, 40, 768))\n",
    "test_eq(cross_attn_weight.shape, (3, 40, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_data.core.ipynb.\n",
      "Converted 02_pytorch.transformer.ipynb.\n",
      "Converted 03_pytorch.model.ipynb.\n",
      "Converted 04_optuna.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
